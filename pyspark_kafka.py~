from pyspark.sql.functions import *
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType
from datetime import datetime
from kafka_config import kafka_config

# Define your name for Kafka topics
MY_NAME = "Viacheslav"

# Create Spark Session
spark = SparkSession.builder \
    .appName("SensorDataAggregation") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/checkpoint") \
    .getOrCreate()

# Define schema for incoming JSON data
schema = StructType([
    StructField("sensor_id", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("temperature", DoubleType(), True),
    StructField("humidity", DoubleType(), True)
])

# Read alert conditions from CSV using the correct column names from your file
alerts_conditions = spark.read.csv("alerts_conditions.csv", header=True, inferSchema=True)
# Cache the small dataframe since we'll reuse it
alerts_conditions.cache()

# Read from Kafka
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "77.81.230.104:9092") \
    .option("kafka.security.protocol", "SASL_PLAINTEXT") \
    .option("kafka.sasl.mechanism", "PLAIN") \
    .option("kafka.sasl.jaas.config",
            "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"admin\" password=\"VawEzo1ikLtrA8Ug8THa\";") \
    .option("subscribe", f"building_sensors_{MY_NAME}") \
    .load()

# Parse JSON data
parsed_df = df \
    .selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select(
    col("data.sensor_id").alias("sensor_id"),
    col("data.timestamp").cast("timestamp").alias("timestamp"),
    col("data.temperature").alias("temperature"),
    col("data.humidity").alias("humidity")
)

# Apply watermark to handle late data (10 seconds)
df_with_watermark = parsed_df.withWatermark("timestamp", "10 seconds")

# Apply sliding window aggregation (1 minute window, sliding every 30 seconds)
windowed_aggregates = df_with_watermark \
    .groupBy(window("timestamp", "1 minute", "30 seconds")) \
    .agg(
    avg("temperature").alias("avg_temperature"),
    avg("humidity").alias("avg_humidity")
)

# Broadcast alerts for efficient processing
alert_broadcast = spark.sparkContext.broadcast(alerts_conditions.collect())


# Function to check if an aggregation meets alert criteria with updated column names
def check_alerts(window_start, window_end, avg_temp, avg_humid):
    matched_alerts = []

    for alert in alert_broadcast.value:
        # Проверка условий температуры (используем temperature_min и temperature_max)
        temp_min_alert = alert.temperature_min != -999 and avg_temp < alert.temperature_min
        temp_max_alert = alert.temperature_max != -999 and avg_temp > alert.temperature_max

        # Проверка условий влажности (используем humidity_min и humidity_max)
        humid_min_alert = alert.humidity_min != -999 and avg_humid < alert.humidity_min
        humid_max_alert = alert.humidity_max != -999 and avg_humid > alert.humidity_max

        if temp_min_alert or temp_max_alert or humid_min_alert or humid_max_alert:
            matched_alerts.append({
                "window": {
                    "start": window_start.isoformat(),
                    "end": window_end.isoformat()
                },
                "t_avg": float(avg_temp),
                "h_avg": float(avg_humid),
                "code": str(alert.code),
                "message": alert.message,
                "timestamp": datetime.now().isoformat(' ')
            })

    return matched_alerts


# Process each window and check against alert conditions
def process_batch(batch_df, batch_id):
    if batch_df.rdd.isEmpty():
        print("Empty batch, no data to process")
        return

    # Create a dataframe with alerts format
    for row in batch_df.collect():
        window_data = row.window
        avg_temp = row.avg_temperature
        avg_humid = row.avg_humidity

        alerts = check_alerts(window_data.start, window_data.end, avg_temp, avg_humid)

        for alert in alerts:
            # Create a dataframe for this alert and write to Kafka
            alert_df = spark.createDataFrame([alert])

            # Convert to JSON format for Kafka
            kafka_df = alert_df.select(
                to_json(struct("*")).alias("value")
            )

            # Write to Kafka
            kafka_df.write \
                .format("kafka") \
                .option("kafka.bootstrap.servers", "77.81.230.104:9092") \
                .option("kafka.security.protocol", "SASL_PLAINTEXT") \
                .option("kafka.sasl.mechanism", "PLAIN") \
                .option("kafka.sasl.jaas.config",
                        "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"admin\" password=\"VawEzo1ikLtrA8Ug8THa\";") \
                .option("topic", f"aggregated_alerts_{MY_NAME}") \
                .save()

            print(f"Alert generated: {alert}")


# Start stream processing
query = windowed_aggregates \
    .writeStream \
    .foreachBatch(process_batch) \
    .outputMode("update") \
    .start()

# Wait for termination
query.awaitTermination()